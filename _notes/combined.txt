October 15
	
	find some obituaries of people from Abdullah's list
	detect businesses from the text
		did I lose all the stuff I've done...?
		maybe it's all in github...

	memes


make practical accomplishments for strang.
for example, I'm probably writing this methodological paper, so get cracking! have you really understood anything yet?
why do I know nothing.
I need some solid steps forward I can make instead of just staring at code that doesn't work.
small, baby steps, that are practical
like the Jupyter notebook

btw small step: jupyter notebook

I should do my work 4 small steps at a time. baby steps, as efficient as possible.
if not possible, push it off (more accurately, brainstorm about it and then push it off)

for the meme analysis:
just ignore those words which are super infrequent --
	well, don't ignore them, but replace them with a null character while counting!

machine-learning ideas
	it'd be good to present the current state of the machine-learning endeavor to the group schematically.
		helping them to understand the steps I'm undertaking could help them contribute in meaningful ways
		I need good ideas about features to include in the algorithm, new ways of structuring the algorithm, and other data to take into consideration

make sure the jupyter notebook is really what's going on
	they don't have the codings!!
	and code everything

give them the actual count of codings for OCC code
	to see if the distribution is the same

inter-coder reliability by OCC code

false and false positives for full body
try parsing to focus *where* occupation words are likely to show up
	maybe loking at the "change" between two methods (how many more false positives / errors / w/e)

verbs for classification?
revive the wikidata linkage
supergroup inter-coder agreement
network
	probably linked to people who do all kinds of things --
	what about multimodal ties?
		based on context of the name
	and people in certain structural relations
documentation
	load a CSV
	get attributes

what can Lily do?
ideas for Lily
	features / facts which can be extracted from the text

	comparing two groups of obituaries for the differences in language

	finding memes which emerge in the obituaries, and persist
		which memes co-occur?

	clustering 
		of documents
		of sentences
		of "what they did" words

		via word-cooccurrence / LDA
		maybe I need something simpler
			which I can explain myself lol

	eliminating duplicates

	when and where did they die?

Gary King
	aggregates can do a good job, even though individual accuracy not so good

ingest earlier obituaries

last name + date of death name search

get abdullah's 200 001's

basis for assessing language change over time

how much inter-coder agreement do we need?

attention model
	within quotes simply excluded
	the relations to words around them

can I automatically determine varying levels of abstraction
	what does this even mean?
	from a note on a piece of paper

as input to machine learning:
	co-occurrence of lists of organizations
	also, links based on names that co-occur

career, as sequence of occupations

read quant-linguistics papers and apply them
	in the spirit of NLP & social interaction

length of obit by gender, OCC, age, words in the article, etc.

M/F which words were used?

more accurate name detection.
more comprehensive 1st sentence fixes
going back into original files themselves
	cleaning up the cleaning code,
	re-running the cleaning scripts, fixing the small errors we've seen (and retrieving paragraphs)
domains	
Occ roles linked to name (Adm. Dewey, The Rev. Upright, Dr. Doolittle)
Eliminate occupations that are not the obituarized: “secretary to the president”, “he once met the president”, “medical examiner says he is dead”
Tough: actors described as stars, but played a doctor on TV --
	we should explore and test the "director of X", "authority on X" coding strategy

analysis of fit for supergroups

1) fixing small errors ... 
deleting the "not an obit" cases (there are 6 of these), 
for purposes of occ testing, combine 001a with 001.

2) employ the occ automated coding to mirror as closely as possible the method that abdullah and I settled on for hand coding (where the basic idea was to have limited, targeted examination of the full body) 

	what we did
	a) always examine both title and first sentence for occ vocabulary
	b) if no terms found in title and first sentence, examine the full body of the article for any occ terms 
	c) if find a supergroup but not 3 digit occ in the title or first sentence, examine the full body to disambiguate that supergroup (see if can code at 3 digit level), but modify any other codes

	as I think about it, not easy to mirror this since we didnt add any info to distinguish whether looked at the full body or not ... best first approx is to only look at title and first sentence, maybe can improve on this later!

3) how to assess fit...

	in the output you sent by email, you show "fit" at the 3 digit level, like "001: missed 44/151". I assume this means that there are 151 appearances of 001 in the consensus hand coded 1000 documents, and 107 of these were identified as 001 in our occ vocabulary while 44 were not identified as 001

	would be good also to look at the flipside, for example, how many documents were auto coded as 001, and how many of these did the consensus hand coding call 001... informative perhaps to have a table where 3 digit occs are the rows, and columns are (a) number appearing in hand coded (b) number in auto occ coded (c) number in both hand and auto occ coded

	would be good also to have the same kind of table but at the supergroup level of granularity... combining those actually coded as supergroups and also their 3 digit members
